{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "36e0e651-53d3-4dab-8c5f-4ae1914ef5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess # Converte um documento em uma lista de tokens\n",
    "from gensim.corpora import Dictionary # Vai mapear uma palavra tokenizada a um ID unico\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8c9d87-95f6-4079-bcc1-729cdeb674cc",
   "metadata": {},
   "source": [
    "# Bag of words with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2bbef06-a2d0-470b-af2d-f3d02d9364b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = [\n",
    "\"Start spreading the news\",\n",
    "\"You're leaving today (tell him friend)\",\n",
    "\"I want to be a part of it, New York, New York\",\n",
    "\"Your vagabond shoes, they are longing to stray\",\n",
    "\"And steps around the heart of it, New York, New York\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f96dfbd9-81a9-4984-bfab-41223e9e84b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Start spreading the news',\n",
      " \"You're leaving today (tell him friend)\",\n",
      " 'I want to be a part of it, New York, New York',\n",
      " 'Your vagabond shoes, they are longing to stray',\n",
      " 'And steps around the heart of it, New York, New York']\n"
     ]
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "pp.pprint(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ed0c048-c60c-470b-ad84-d2743dd2a903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['start', 'spreading', 'the', 'news'],\n",
       " ['you', 're', 'leaving', 'today', 'tell', 'him', 'friend'],\n",
       " ['want', 'to', 'be', 'part', 'of', 'it', 'new', 'york', 'new', 'york'],\n",
       " ['your', 'vagabond', 'shoes', 'they', 'are', 'longing', 'to', 'stray'],\n",
       " ['and',\n",
       "  'steps',\n",
       "  'around',\n",
       "  'the',\n",
       "  'heart',\n",
       "  'of',\n",
       "  'it',\n",
       "  'new',\n",
       "  'york',\n",
       "  'new',\n",
       "  'york']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokenized = [simple_preprocess(doc) for doc in doc_list]\n",
    "doc_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65a28856-02b3-4e4a-a9e5-7de01c9ea0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x710459778390>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = Dictionary()\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4bce0ba-b642-4a78-a644-5fce0c8a206e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
       " [(11, 1), (12, 1), (13, 2), (14, 1), (15, 1), (16, 1), (17, 1), (18, 2)],\n",
       " [(16, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1)],\n",
       " [(3, 1),\n",
       "  (12, 1),\n",
       "  (13, 2),\n",
       "  (14, 1),\n",
       "  (18, 2),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1)]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estou associando um id a cada token e uma cont√°gem para quantas vezes ele aparece\n",
    "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
    "BoW_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27722fbd-5106-49d6-a68e-6ec606c07ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('news', 1), ('spreading', 1), ('start', 1), ('the', 1)],\n",
      " [('friend', 1),\n",
      "  ('him', 1),\n",
      "  ('leaving', 1),\n",
      "  ('re', 1),\n",
      "  ('tell', 1),\n",
      "  ('today', 1),\n",
      "  ('you', 1)],\n",
      " [('be', 1),\n",
      "  ('it', 1),\n",
      "  ('new', 2),\n",
      "  ('of', 1),\n",
      "  ('part', 1),\n",
      "  ('to', 1),\n",
      "  ('want', 1),\n",
      "  ('york', 2)],\n",
      " [('to', 1),\n",
      "  ('are', 1),\n",
      "  ('longing', 1),\n",
      "  ('shoes', 1),\n",
      "  ('stray', 1),\n",
      "  ('they', 1),\n",
      "  ('vagabond', 1),\n",
      "  ('your', 1)],\n",
      " [('the', 1),\n",
      "  ('it', 1),\n",
      "  ('new', 2),\n",
      "  ('of', 1),\n",
      "  ('york', 2),\n",
      "  ('and', 1),\n",
      "  ('around', 1),\n",
      "  ('heart', 1),\n",
      "  ('steps', 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Substituindo os ids em cada tupla de 2 pelo token em si\n",
    "id_words = [[(dictionary[id], count) for id, count in line] for line in BoW_corpus]\n",
    "pp.pprint(id_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ba53b-5c3b-496d-83ca-9cf03a164f90",
   "metadata": {},
   "source": [
    "# Bag of Words with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4dcadaf-0d29-4792-a29a-12715c14fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importanto o modelo CointVectorizer para converter os tokens do documento em uma matriz de contagem de tokens\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fb2bf7f-68f4-441f-a6e9-00e6c0fd6f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o modelo\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98e5a259-d4ae-48cf-8270-4147160cec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando e transformando o modelo\n",
    "cv_fit = cv.fit_transform(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbe1981a-835d-4426-89f7-cce677470aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'are', 'around', 'be', 'friend', 'heart', 'him', 'it',\n",
       "       'leaving', 'longing', 'new', 'news', 'of', 'part', 're', 'shoes',\n",
       "       'spreading', 'start', 'steps', 'stray', 'tell', 'the', 'they',\n",
       "       'to', 'today', 'vagabond', 'want', 'york', 'you', 'your'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrando as palavras em uma lista\n",
    "word_list = cv.get_feature_names_out()\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8ee46d0-7c2d-4f39-bc70-845317721208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 2, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 0, 1, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 2, 0, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_fit.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1709dd23-f613-4c9d-99ea-39b487ae5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_list = cv_fit.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "705844e8-0964-4356-940d-ca59ba1f66eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1,\n",
      " 'are': 1,\n",
      " 'around': 1,\n",
      " 'be': 1,\n",
      " 'friend': 1,\n",
      " 'heart': 1,\n",
      " 'him': 1,\n",
      " 'it': 2,\n",
      " 'leaving': 1,\n",
      " 'longing': 1,\n",
      " 'new': 4,\n",
      " 'news': 1,\n",
      " 'of': 2,\n",
      " 'part': 1,\n",
      " 're': 1,\n",
      " 'shoes': 1,\n",
      " 'spreading': 1,\n",
      " 'start': 1,\n",
      " 'steps': 1,\n",
      " 'stray': 1,\n",
      " 'tell': 1,\n",
      " 'the': 2,\n",
      " 'they': 1,\n",
      " 'to': 2,\n",
      " 'today': 1,\n",
      " 'vagabond': 1,\n",
      " 'want': 1,\n",
      " 'york': 4,\n",
      " 'you': 1,\n",
      " 'your': 1}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint( dict(zip(word_list,count_list)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fd2e06-5d3d-4127-9031-aee4d4c0fb06",
   "metadata": {},
   "source": [
    "# Bag of N-Grams with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55cddb7a-61d0-4aa3-91c6-cb83a5ab799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando os bigrams com gensim\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24beba89-69b2-49eb-8bc7-2ec8290acca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list_Ngrams = [\n",
    "\"Start spreading the news\",\n",
    "\"You're leaving today\",\n",
    "\"I want to be a part of it, New York, New York\",\n",
    "\"Your vagabond shoes, they are longing to stray\",\n",
    "\"And steps around the heart of it, New York, New York\",\n",
    "\"Come and visit us\",\n",
    "\"Come and visit the city\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d47c883f-84a9-44b3-8b7d-89e91d7746cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['start', 'spreading', 'the', 'news'],\n",
       " ['you', 're', 'leaving', 'today'],\n",
       " ['want', 'to', 'be', 'part', 'of', 'it', 'new', 'york', 'new', 'york'],\n",
       " ['your', 'vagabond', 'shoes', 'they', 'are', 'longing', 'to', 'stray'],\n",
       " ['and',\n",
       "  'steps',\n",
       "  'around',\n",
       "  'the',\n",
       "  'heart',\n",
       "  'of',\n",
       "  'it',\n",
       "  'new',\n",
       "  'york',\n",
       "  'new',\n",
       "  'york'],\n",
       " ['come', 'and', 'visit', 'us'],\n",
       " ['come', 'and', 'visit', 'the', 'city']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizando \n",
    "doc_tokenized_Ngrams = [simple_preprocess(doc) for doc in doc_list_Ngrams]\n",
    "doc_tokenized_Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "319be9f7-3eb1-40c1-aef5-1f0bd52233ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rodando a classe Prases para separar alguns tokens em bigrams mais comuns\n",
    "bigram = Phrases(doc_tokenized_Ngrams, min_count=1, threshold=2, delimiter=' ')\n",
    "bigram_phraser = Phraser(bigram)\n",
    "doc_bigrams = [bigram_phraser[doc] for doc in doc_tokenized_Ngrams]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7fde076-86ce-4722-bfb6-8e57e8955459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start', 'spreading', 'the', 'news']\n",
      "['you', 're', 'leaving', 'today']\n",
      "['want', 'to', 'be', 'part', 'of it', 'new york', 'new york']\n",
      "['your', 'vagabond', 'shoes', 'they', 'are', 'longing', 'to', 'stray']\n",
      "['and', 'steps', 'around', 'the', 'heart', 'of it', 'new york', 'new york']\n",
      "['come and', 'visit', 'us']\n",
      "['come and', 'visit', 'the', 'city']\n"
     ]
    }
   ],
   "source": [
    "for sent in doc_tokenized_Ngrams:\n",
    "    tokens_ = bigram_phraser[sent]\n",
    "    print(tokens_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "488c90b1-0d20-422a-9c53-113dff628c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = Phrases(bigram[doc_tokenized_Ngrams], min_count=1, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c19f44a-eac2-44a0-8bd7-73e0c03480fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['come and visit']\n",
      "['come and visit']\n"
     ]
    }
   ],
   "source": [
    "for sent in doc_tokenized_Ngrams:\n",
    "    bigrams_ = [b for b in bigram[sent] if b.count(' ') ==1]\n",
    "    trigrams_ = [t for t in trigram[bigram[sent]] if t.count(' ') == 2]\n",
    "    print(trigrams_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f21222-100e-467a-a70c-e22b3746218b",
   "metadata": {},
   "source": [
    "# Bag of N-Grams Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "832c0901-86ea-41b9-85fa-0a7f8a2f4797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando e treinando o modelo para gerar os bigrams\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "ngram_fit = ngram_vectorizer.fit_transform(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1e4683c-b3f4-4b3e-8708-a687b81bb5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and steps', 'are longing', 'around the', 'be part', 'heart of',\n",
       "       'him friend', 'it new', 'leaving today', 'longing to', 'new york',\n",
       "       'of it', 'part of', 're leaving', 'shoes they', 'spreading the',\n",
       "       'start spreading', 'steps around', 'tell him', 'the heart',\n",
       "       'the news', 'they are', 'to be', 'to stray', 'today tell',\n",
       "       'vagabond shoes', 'want to', 'york new', 'you re', 'your vagabond'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando um array com os tokens\n",
    "word_list = ngram_vectorizer.get_feature_names_out()\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9401a4a2-8f02-4917-b3d9-bb3c45f2822c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 2, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando um array com a contagem de palavras\n",
    "count_list = ngram_fit.toarray().sum(axis=0)\n",
    "count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99be1b0d-a310-4300-b95f-0ae4166b6624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and steps': 1,\n",
      " 'are longing': 1,\n",
      " 'around the': 1,\n",
      " 'be part': 1,\n",
      " 'heart of': 1,\n",
      " 'him friend': 1,\n",
      " 'it new': 2,\n",
      " 'leaving today': 1,\n",
      " 'longing to': 1,\n",
      " 'new york': 4,\n",
      " 'of it': 2,\n",
      " 'part of': 1,\n",
      " 're leaving': 1,\n",
      " 'shoes they': 1,\n",
      " 'spreading the': 1,\n",
      " 'start spreading': 1,\n",
      " 'steps around': 1,\n",
      " 'tell him': 1,\n",
      " 'the heart': 1,\n",
      " 'the news': 1,\n",
      " 'they are': 1,\n",
      " 'to be': 1,\n",
      " 'to stray': 1,\n",
      " 'today tell': 1,\n",
      " 'vagabond shoes': 1,\n",
      " 'want to': 1,\n",
      " 'york new': 2,\n",
      " 'you re': 1,\n",
      " 'your vagabond': 1}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(dict(zip(word_list, count_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b31e020-abc0-4aec-92a7-6d5119d1d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "ngram_fit = ngram_vectorizer.fit_transform(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e393096-25ad-4a96-abf6-93e781535db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and steps around': 1,\n",
      " 'are longing to': 1,\n",
      " 'around the heart': 1,\n",
      " 'be part of': 1,\n",
      " 'heart of it': 1,\n",
      " 'it new york': 2,\n",
      " 'leaving today tell': 1,\n",
      " 'longing to stray': 1,\n",
      " 'new york new': 2,\n",
      " 'of it new': 2,\n",
      " 'part of it': 1,\n",
      " 're leaving today': 1,\n",
      " 'shoes they are': 1,\n",
      " 'spreading the news': 1,\n",
      " 'start spreading the': 1,\n",
      " 'steps around the': 1,\n",
      " 'tell him friend': 1,\n",
      " 'the heart of': 1,\n",
      " 'they are longing': 1,\n",
      " 'to be part': 1,\n",
      " 'today tell him': 1,\n",
      " 'vagabond shoes they': 1,\n",
      " 'want to be': 1,\n",
      " 'york new york': 2,\n",
      " 'you re leaving': 1,\n",
      " 'your vagabond shoes': 1}\n"
     ]
    }
   ],
   "source": [
    "word_list = ngram_vectorizer.get_feature_names_out()\n",
    "count_list = ngram_fit.toarray().sum(axis=0)\n",
    "pp.pprint( dict(zip(word_list,count_list)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236c77d1-4916-4266-bce0-757d313dd19e",
   "metadata": {},
   "source": [
    "# NLTK for N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fffbe58e-cea5-47c6-acaa-9ee85c36a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8b432f84-30f2-4029-bc84-29259b96a349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start',\n",
       " 'spreading',\n",
       " 'the',\n",
       " 'news',\n",
       " 'you',\n",
       " 're',\n",
       " 'leaving',\n",
       " 'today',\n",
       " 'tell',\n",
       " 'him',\n",
       " 'friend',\n",
       " 'want',\n",
       " 'to',\n",
       " 'be',\n",
       " 'part',\n",
       " 'of',\n",
       " 'it',\n",
       " 'new',\n",
       " 'york',\n",
       " 'new',\n",
       " 'york',\n",
       " 'your',\n",
       " 'vagabond',\n",
       " 'shoes',\n",
       " 'they',\n",
       " 'are',\n",
       " 'longing',\n",
       " 'to',\n",
       " 'stray',\n",
       " 'and',\n",
       " 'steps',\n",
       " 'around',\n",
       " 'the',\n",
       " 'heart',\n",
       " 'of',\n",
       " 'it',\n",
       " 'new',\n",
       " 'york',\n",
       " 'new',\n",
       " 'york']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Colocando todas as palavras de todas as frases numa lista s√≥\n",
    "flat_list = []\n",
    "for sublist in doc_tokenized:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69942f65-28d1-40e9-9043-bc31e4bd6b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('start', 'spreading'),\n",
       " ('spreading', 'the'),\n",
       " ('the', 'news'),\n",
       " ('news', 'you'),\n",
       " ('you', 're'),\n",
       " ('re', 'leaving'),\n",
       " ('leaving', 'today'),\n",
       " ('today', 'tell'),\n",
       " ('tell', 'him'),\n",
       " ('him', 'friend'),\n",
       " ('friend', 'want'),\n",
       " ('want', 'to'),\n",
       " ('to', 'be'),\n",
       " ('be', 'part'),\n",
       " ('part', 'of'),\n",
       " ('of', 'it'),\n",
       " ('it', 'new'),\n",
       " ('new', 'york'),\n",
       " ('york', 'new'),\n",
       " ('new', 'york'),\n",
       " ('york', 'your'),\n",
       " ('your', 'vagabond'),\n",
       " ('vagabond', 'shoes'),\n",
       " ('shoes', 'they'),\n",
       " ('they', 'are'),\n",
       " ('are', 'longing'),\n",
       " ('longing', 'to'),\n",
       " ('to', 'stray'),\n",
       " ('stray', 'and'),\n",
       " ('and', 'steps'),\n",
       " ('steps', 'around'),\n",
       " ('around', 'the'),\n",
       " ('the', 'heart'),\n",
       " ('heart', 'of'),\n",
       " ('of', 'it'),\n",
       " ('it', 'new'),\n",
       " ('new', 'york'),\n",
       " ('york', 'new'),\n",
       " ('new', 'york')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gerando bigrams com o nltk\n",
    "nltk_bigrams = list(bigrams(flat_list))\n",
    "nltk_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f10972ef-e0aa-4b93-b307-bb94f764ab10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('start', 'spreading', 'the'),\n",
       " ('spreading', 'the', 'news'),\n",
       " ('the', 'news', 'you'),\n",
       " ('news', 'you', 're'),\n",
       " ('you', 're', 'leaving'),\n",
       " ('re', 'leaving', 'today'),\n",
       " ('leaving', 'today', 'tell'),\n",
       " ('today', 'tell', 'him'),\n",
       " ('tell', 'him', 'friend'),\n",
       " ('him', 'friend', 'want'),\n",
       " ('friend', 'want', 'to'),\n",
       " ('want', 'to', 'be'),\n",
       " ('to', 'be', 'part'),\n",
       " ('be', 'part', 'of'),\n",
       " ('part', 'of', 'it'),\n",
       " ('of', 'it', 'new'),\n",
       " ('it', 'new', 'york'),\n",
       " ('new', 'york', 'new'),\n",
       " ('york', 'new', 'york'),\n",
       " ('new', 'york', 'your'),\n",
       " ('york', 'your', 'vagabond'),\n",
       " ('your', 'vagabond', 'shoes'),\n",
       " ('vagabond', 'shoes', 'they'),\n",
       " ('shoes', 'they', 'are'),\n",
       " ('they', 'are', 'longing'),\n",
       " ('are', 'longing', 'to'),\n",
       " ('longing', 'to', 'stray'),\n",
       " ('to', 'stray', 'and'),\n",
       " ('stray', 'and', 'steps'),\n",
       " ('and', 'steps', 'around'),\n",
       " ('steps', 'around', 'the'),\n",
       " ('around', 'the', 'heart'),\n",
       " ('the', 'heart', 'of'),\n",
       " ('heart', 'of', 'it'),\n",
       " ('of', 'it', 'new'),\n",
       " ('it', 'new', 'york'),\n",
       " ('new', 'york', 'new'),\n",
       " ('york', 'new', 'york')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gerando trigrams com o nltk\n",
    "nltk_trigrams = list(trigrams(flat_list))\n",
    "nltk_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903b79de-7cd2-48c2-ad52-f6aefc9e3124",
   "metadata": {},
   "source": [
    "# Gensim for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f6d69cf8-ed32-4669-8685-8cd2f6605a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "tfidf = TfidfModel(BoW_corpus, smartirs='ntc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "042a672d-0410-40ed-99ae-ced997551bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.5442545024816783),\n",
       "  (1, 0.5442545024816783),\n",
       "  (2, 0.5442545024816783),\n",
       "  (3, 0.33370812034660224)],\n",
       " [(4, 0.3779644730092272),\n",
       "  (5, 0.3779644730092272),\n",
       "  (6, 0.3779644730092272),\n",
       "  (7, 0.3779644730092272),\n",
       "  (8, 0.3779644730092272),\n",
       "  (9, 0.3779644730092272),\n",
       "  (10, 0.3779644730092272)],\n",
       " [(11, 0.3743600446812478),\n",
       "  (12, 0.22953781047985866),\n",
       "  (13, 0.4590756209597173),\n",
       "  (14, 0.22953781047985866),\n",
       "  (15, 0.3743600446812478),\n",
       "  (16, 0.22953781047985866),\n",
       "  (17, 0.3743600446812478),\n",
       "  (18, 0.4590756209597173)],\n",
       " [(16, 0.22576456473655607),\n",
       "  (19, 0.36820614593095874),\n",
       "  (20, 0.36820614593095874),\n",
       "  (21, 0.36820614593095874),\n",
       "  (22, 0.36820614593095874),\n",
       "  (23, 0.36820614593095874),\n",
       "  (24, 0.36820614593095874),\n",
       "  (25, 0.36820614593095874)],\n",
       " [(3, 0.21496814396163463),\n",
       "  (12, 0.21496814396163463),\n",
       "  (13, 0.42993628792326927),\n",
       "  (14, 0.21496814396163463),\n",
       "  (18, 0.42993628792326927),\n",
       "  (26, 0.35059794205706235),\n",
       "  (27, 0.35059794205706235),\n",
       "  (28, 0.35059794205706235),\n",
       "  (29, 0.35059794205706235)]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando uma lista de tuplas onde o valor do primeiro elemento das tuplas √© o n√∫mero de cada palavra e o segundo o valor de cada palavra no corpus\n",
    "list(tfidf[BoW_corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04d1858-6fad-4c6c-b3ea-1ed096bb44ce",
   "metadata": {},
   "source": [
    "# scikit-learn for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a758c11f-d0cd-439a-8746-d4ce41f2433e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1c2ed0c4-6324-47e8-905f-056626b089ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.52 0.   0.\n",
      "  0.   0.   0.52 0.52 0.   0.   0.   0.42 0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.38 0.   0.38 0.   0.38 0.   0.   0.   0.   0.\n",
      "  0.38 0.   0.   0.   0.   0.   0.38 0.   0.   0.   0.38 0.   0.   0.\n",
      "  0.38 0.  ]\n",
      " [0.   0.   0.   0.31 0.   0.   0.   0.25 0.   0.   0.51 0.   0.25 0.31\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.25 0.   0.   0.31 0.51\n",
      "  0.   0.  ]\n",
      " [0.   0.36 0.   0.   0.   0.   0.   0.   0.   0.36 0.   0.   0.   0.\n",
      "  0.   0.36 0.   0.   0.   0.36 0.   0.   0.36 0.29 0.   0.36 0.   0.\n",
      "  0.   0.36]\n",
      " [0.3  0.   0.3  0.   0.   0.3  0.   0.24 0.   0.   0.48 0.   0.24 0.\n",
      "  0.   0.   0.   0.   0.3  0.   0.   0.24 0.   0.   0.   0.   0.   0.48\n",
      "  0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "print(tfidf_vectorizer.transform(doc_list).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4672f2d4-20e0-4595-aae6-41e6dbe0513d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'are', 'around', 'be', 'friend', 'heart', 'him', 'it',\n",
       "       'leaving', 'longing', 'new', 'news', 'of', 'part', 're', 'shoes',\n",
       "       'spreading', 'start', 'steps', 'stray', 'tell', 'the', 'they',\n",
       "       'to', 'today', 'vagabond', 'want', 'york', 'you', 'your'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "254d6d46-fe19-4810-b4af-c540e4c8b8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>around</th>\n",
       "      <th>be</th>\n",
       "      <th>friend</th>\n",
       "      <th>heart</th>\n",
       "      <th>him</th>\n",
       "      <th>it</th>\n",
       "      <th>leaving</th>\n",
       "      <th>longing</th>\n",
       "      <th>...</th>\n",
       "      <th>tell</th>\n",
       "      <th>the</th>\n",
       "      <th>they</th>\n",
       "      <th>to</th>\n",
       "      <th>today</th>\n",
       "      <th>vagabond</th>\n",
       "      <th>want</th>\n",
       "      <th>york</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.422242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313727</td>\n",
       "      <td>0.506225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361529</td>\n",
       "      <td>0.291679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.299341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.299341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.299341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241507</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241507</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        and       are    around        be    friend     heart       him  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.377964  0.000000  0.377964   \n",
       "2  0.000000  0.000000  0.000000  0.313727  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.361529  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.299341  0.000000  0.299341  0.000000  0.000000  0.299341  0.000000   \n",
       "\n",
       "         it   leaving   longing  ...      tell       the      they        to  \\\n",
       "0  0.000000  0.000000  0.000000  ...  0.000000  0.422242  0.000000  0.000000   \n",
       "1  0.000000  0.377964  0.000000  ...  0.377964  0.000000  0.000000  0.000000   \n",
       "2  0.253113  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.253113   \n",
       "3  0.000000  0.000000  0.361529  ...  0.000000  0.000000  0.361529  0.291679   \n",
       "4  0.241507  0.000000  0.000000  ...  0.000000  0.241507  0.000000  0.000000   \n",
       "\n",
       "      today  vagabond      want      york       you      your  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.377964  0.000000  0.000000  0.000000  0.377964  0.000000  \n",
       "2  0.000000  0.000000  0.313727  0.506225  0.000000  0.000000  \n",
       "3  0.000000  0.361529  0.000000  0.000000  0.000000  0.361529  \n",
       "4  0.000000  0.000000  0.000000  0.483013  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tfidf_vectorizer.transform(doc_list).toarray(), columns = tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec76f5b-c43f-4d8c-9a80-129708d6a4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
