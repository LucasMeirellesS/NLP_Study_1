{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e0e651-53d3-4dab-8c5f-4ae1914ef5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess # Converte um documento em uma lista de tokens\n",
    "from gensim.corpora import Dictionary # Vai mapear uma palavra tokenizada a um ID unico\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8c9d87-95f6-4079-bcc1-729cdeb674cc",
   "metadata": {},
   "source": [
    "# Bag of words with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2bbef06-a2d0-470b-af2d-f3d02d9364b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = [\n",
    "\"Start spreading the news\",\n",
    "\"You're leaving today (tell him friend)\",\n",
    "\"I want to be a part of it, New York, New York\",\n",
    "\"Your vagabond shoes, they are longing to stray\",\n",
    "\"And steps around the heart of it, New York, New York\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96dfbd9-81a9-4984-bfab-41223e9e84b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Start spreading the news',\n",
      " \"You're leaving today (tell him friend)\",\n",
      " 'I want to be a part of it, New York, New York',\n",
      " 'Your vagabond shoes, they are longing to stray',\n",
      " 'And steps around the heart of it, New York, New York']\n"
     ]
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "pp.pprint(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed0c048-c60c-470b-ad84-d2743dd2a903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['start', 'spreading', 'the', 'news'],\n",
       " ['you', 're', 'leaving', 'today', 'tell', 'him', 'friend'],\n",
       " ['want', 'to', 'be', 'part', 'of', 'it', 'new', 'york', 'new', 'york'],\n",
       " ['your', 'vagabond', 'shoes', 'they', 'are', 'longing', 'to', 'stray'],\n",
       " ['and',\n",
       "  'steps',\n",
       "  'around',\n",
       "  'the',\n",
       "  'heart',\n",
       "  'of',\n",
       "  'it',\n",
       "  'new',\n",
       "  'york',\n",
       "  'new',\n",
       "  'york']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokenized = [simple_preprocess(doc) for doc in doc_list]\n",
    "doc_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65a28856-02b3-4e4a-a9e5-7de01c9ea0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x79a7a2ff8450>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = Dictionary()\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4bce0ba-b642-4a78-a644-5fce0c8a206e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
       " [(11, 1), (12, 1), (13, 2), (14, 1), (15, 1), (16, 1), (17, 1), (18, 2)],\n",
       " [(16, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1)],\n",
       " [(3, 1),\n",
       "  (12, 1),\n",
       "  (13, 2),\n",
       "  (14, 1),\n",
       "  (18, 2),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estou associando um id a cada token e uma cont√°gem para quantas vezes ele aparece\n",
    "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
    "BoW_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27722fbd-5106-49d6-a68e-6ec606c07ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('news', 1), ('spreading', 1), ('start', 1), ('the', 1)],\n",
      " [('friend', 1),\n",
      "  ('him', 1),\n",
      "  ('leaving', 1),\n",
      "  ('re', 1),\n",
      "  ('tell', 1),\n",
      "  ('today', 1),\n",
      "  ('you', 1)],\n",
      " [('be', 1),\n",
      "  ('it', 1),\n",
      "  ('new', 2),\n",
      "  ('of', 1),\n",
      "  ('part', 1),\n",
      "  ('to', 1),\n",
      "  ('want', 1),\n",
      "  ('york', 2)],\n",
      " [('to', 1),\n",
      "  ('are', 1),\n",
      "  ('longing', 1),\n",
      "  ('shoes', 1),\n",
      "  ('stray', 1),\n",
      "  ('they', 1),\n",
      "  ('vagabond', 1),\n",
      "  ('your', 1)],\n",
      " [('the', 1),\n",
      "  ('it', 1),\n",
      "  ('new', 2),\n",
      "  ('of', 1),\n",
      "  ('york', 2),\n",
      "  ('and', 1),\n",
      "  ('around', 1),\n",
      "  ('heart', 1),\n",
      "  ('steps', 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Substituindo os ids em cada tupla de 2 pelo token em si\n",
    "id_words = [[(dictionary[id], count) for id, count in line] for line in BoW_corpus]\n",
    "pp.pprint(id_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ba53b-5c3b-496d-83ca-9cf03a164f90",
   "metadata": {},
   "source": [
    "# Bag of Words with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4dcadaf-0d29-4792-a29a-12715c14fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importanto o modelo CointVectorizer para converter os tokens do documento em uma matriz de contagem de tokens\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fb2bf7f-68f4-441f-a6e9-00e6c0fd6f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o modelo\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98e5a259-d4ae-48cf-8270-4147160cec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokenized_Ngrams# Treinando e transformando o modelo\n",
    "cv_fit = cv.fit_transform(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbe1981a-835d-4426-89f7-cce677470aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'are', 'around', 'be', 'friend', 'heart', 'him', 'it',\n",
       "       'leaving', 'longing', 'new', 'news', 'of', 'part', 're', 'shoes',\n",
       "       'spreading', 'start', 'steps', 'stray', 'tell', 'the', 'they',\n",
       "       'to', 'today', 'vagabond', 'want', 'york', 'you', 'your'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrando as palavras em uma lista\n",
    "word_list = cv.get_feature_names_out()\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8ee46d0-7c2d-4f39-bc70-845317721208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 2, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 0, 1, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 2, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_fit.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1709dd23-f613-4c9d-99ea-39b487ae5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_list = cv_fit.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "705844e8-0964-4356-940d-ca59ba1f66eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1,\n",
      " 'are': 1,\n",
      " 'around': 1,\n",
      " 'be': 1,\n",
      " 'friend': 1,\n",
      " 'heart': 1,\n",
      " 'him': 1,\n",
      " 'it': 2,\n",
      " 'leaving': 1,\n",
      " 'longing': 1,\n",
      " 'new': 4,\n",
      " 'news': 1,\n",
      " 'of': 2,\n",
      " 'part': 1,\n",
      " 're': 1,\n",
      " 'shoes': 1,\n",
      " 'spreading': 1,\n",
      " 'start': 1,\n",
      " 'steps': 1,\n",
      " 'stray': 1,\n",
      " 'tell': 1,\n",
      " 'the': 2,\n",
      " 'they': 1,\n",
      " 'to': 2,\n",
      " 'today': 1,\n",
      " 'vagabond': 1,\n",
      " 'want': 1,\n",
      " 'york': 4,\n",
      " 'you': 1,\n",
      " 'your': 1}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint( dict(zip(word_list,count_list)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fd2e06-5d3d-4127-9031-aee4d4c0fb06",
   "metadata": {},
   "source": [
    "# Bag of N-Grams with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55cddb7a-61d0-4aa3-91c6-cb83a5ab799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando os bigrams com gensim\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24beba89-69b2-49eb-8bc7-2ec8290acca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list_Ngrams = [\n",
    "\"Start spreading the news\",\n",
    "\"You're leaving today\",\n",
    "\"I want to be a part of it, New York, New York\",\n",
    "\"Your vagabond shoes, they are longing to stray\",\n",
    "\"And steps around the heart of it, New York, New York\",\n",
    "\"Come and visit us\",\n",
    "\"Come and visit the city\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d47c883f-84a9-44b3-8b7d-89e91d7746cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['start', 'spreading', 'the', 'news'],\n",
       " ['you', 're', 'leaving', 'today'],\n",
       " ['want', 'to', 'be', 'part', 'of', 'it', 'new', 'york', 'new', 'york'],\n",
       " ['your', 'vagabond', 'shoes', 'they', 'are', 'longing', 'to', 'stray'],\n",
       " ['and',\n",
       "  'steps',\n",
       "  'around',\n",
       "  'the',\n",
       "  'heart',\n",
       "  'of',\n",
       "  'it',\n",
       "  'new',\n",
       "  'york',\n",
       "  'new',\n",
       "  'york'],\n",
       " ['come', 'and', 'visit', 'us'],\n",
       " ['come', 'and', 'visit', 'the', 'city']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizando \n",
    "doc_tokenized_Ngrams = [simple_preprocess(doc) for doc in doc_list_Ngrams]\n",
    "doc_tokenized_Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "319be9f7-3eb1-40c1-aef5-1f0bd52233ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bigram = Phrases(doc_tokenized_Ngrams, min_count=1, threshold=2, delimiter=' ')\n",
    "bigram_phraser = Phraser(bigram)\n",
    "doc_bigrams = [bigram_phraser[doc] for doc in doc_tokenized_Ngrams]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7fde076-86ce-4722-bfb6-8e57e8955459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start', 'spreading', 'the', 'news']\n",
      "['you', 're', 'leaving', 'today']\n",
      "['want', 'to', 'be', 'part', 'of it', 'new york', 'new york']\n",
      "['your', 'vagabond', 'shoes', 'they', 'are', 'longing', 'to', 'stray']\n",
      "['and', 'steps', 'around', 'the', 'heart', 'of it', 'new york', 'new york']\n",
      "['come and', 'visit', 'us']\n",
      "['come and', 'visit', 'the', 'city']\n"
     ]
    }
   ],
   "source": [
    "for sent in doc_tokenized_Ngrams:\n",
    "    tokens_ = bigram_phraser[sent]\n",
    "    print(tokens_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488c90b1-0d20-422a-9c53-113dff628c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
